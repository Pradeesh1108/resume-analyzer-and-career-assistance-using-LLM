{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-16T16:53:05.550989Z",
     "start_time": "2024-11-16T16:53:05.536859Z"
    }
   },
   "source": [
    "import PyPDF2\n",
    "import docx2txt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:53:08.690363Z",
     "start_time": "2024-11-16T16:53:08.681229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_file = open(file_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in range(len(pdf_reader.pages)):\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        text += page_obj.extract_text()\n",
    "    pdf_file.close()\n",
    "    return text"
   ],
   "id": "74cdbff37d480ebe",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:53:10.876552Z",
     "start_time": "2024-11-16T16:53:10.871116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_docx(file_path):\n",
    "    text = docx2txt.process(file_path)\n",
    "    return text"
   ],
   "id": "460b5e370c1d4c88",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:53:15.534506Z",
     "start_time": "2024-11-16T16:53:15.520864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in word_tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens"
   ],
   "id": "78b465f3effcf573",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:53:31.112614Z",
     "start_time": "2024-11-16T16:53:18.274974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(resume_text, jd_text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([resume_text, jd_text])\n",
    "    similarity_score = cosine_similarity(vectors)[0][1]\n",
    "    return similarity_score"
   ],
   "id": "5126e46dbba991a8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    resume_file_path = \"Dataset/TESTINGFILE1.pdf\"\n",
    "    jd_file_path = \"Dataset/TESTINGFILE2.pdf\"\n",
    "\n",
    "    if resume_file_path.endswith('.pdf'):\n",
    "        resume_text = extract_text_from_pdf(resume_file_path) #pdf\n",
    "        jd_text = extract_text_from_pdf(jd_file_path)\n",
    "    elif resume_file_path.endswith('.docx'):\n",
    "        resume_text = extract_text_from_docx(resume_file_path) #docx\n",
    "        jd_text = extract_text_from_pdf(jd_file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format.\")\n",
    "        return\n",
    "    \n",
    "    preprocessed_resume_text = preprocess_text(resume_text)\n",
    "    preprocessed_jd_text = preprocess_text(jd_text)\n",
    "\n",
    "    similarity_score = calculate_similarity(' '.join(preprocessed_resume_text), ' '.join(preprocessed_jd_text))\n",
    "    print(\"The similarity score between the resume and job description is: \", similarity_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c40e712f5fb22ade"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:54:38.435378Z",
     "start_time": "2024-11-16T16:54:38.378195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    resume_file_path = \"PradeeshResume.pdf\"\n",
    "\n",
    "    # Provide the job description as plain text\n",
    "    jd_text = \"\"\"Amazon is at the forefront of innovative technology, driving progress in AI and machine learning solutions. Our mission is to leverage advanced data science and engineering to deliver impactful and scalable products for our clients. We are looking for a passionate Machine Learning Engineer to join our dynamic team and contribute to cutting-edge projects.\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Design, develop, and deploy scalable machine learning models and algorithms for a variety of applications.\n",
    "Collaborate with cross-functional teams, including data scientists, software engineers, and product managers, to gather requirements and implement solutions.\n",
    "Preprocess and analyze large datasets, ensuring data quality and feature engineering for optimal model performance.\n",
    "Build and maintain machine learning pipelines for training, evaluation, and deployment.\n",
    "Optimize and fine-tune models for performance, scalability, and accuracy using techniques such as hyperparameter tuning and model compression.\n",
    "Conduct thorough testing and validation of models to ensure reliability and robustness in production.\n",
    "Monitor and maintain deployed models, implementing strategies for model retraining and performance tracking.\n",
    "Stay current with the latest advancements in machine learning and AI, applying innovative techniques and technologies as appropriate.\n",
    "Document processes, model architectures, and code to ensure maintainability and knowledge sharing within the team.\n",
    "Qualifications:\n",
    "\n",
    "Bachelor's or Master's degree in Computer Science, AI/ML, Data Science, or a related field.\n",
    "Strong proficiency in Python and experience with libraries/frameworks such as TensorFlow, PyTorch, or Scikit-Learn.\n",
    "Solid understanding of machine learning algorithms, neural networks, and deep learning architectures.\n",
    "Experience with data preprocessing and feature engineering.\n",
    "Hands-on experience with cloud platforms (AWS, GCP, or Azure) for deploying machine learning models.\n",
    "Familiarity with MLOps practices, including version control, CI/CD pipelines, and model monitoring.\n",
    "Excellent problem-solving skills and the ability to work collaboratively in a team environment.\n",
    "Knowledge of big data tools (e.g., Spark, Hadoop) and database technologies is a plus.\n",
    "Nice-to-Have:\n",
    "\n",
    "Experience in Natural Language Processing (NLP) and working with transformers and LLMs.\n",
    "Understanding of computer vision and related frameworks.\n",
    "Contributions to open-source ML projects or participation in hackathons.\n",
    "Perks and Benefits:\n",
    "\n",
    "Competitive salary and performance-based bonuses.\n",
    "Flexible working hours and remote work options.\n",
    "Access to training programs, conferences, and certifications for continuous learning.\n",
    "Comprehensive health and wellness benefits.\n",
    "A collaborative and inclusive work culture with opportunities for career growth.\"\"\"\n",
    "\n",
    "    if resume_file_path.endswith('.pdf'):\n",
    "        resume_text = extract_text_from_pdf(resume_file_path)  # pdf\n",
    "    elif resume_file_path.endswith('.docx'):\n",
    "        resume_text = extract_text_from_docx(resume_file_path)  # docx\n",
    "    else:\n",
    "        print(\"Unsupported file format.\")\n",
    "        return\n",
    "\n",
    "    preprocessed_resume_text = preprocess_text(resume_text)\n",
    "    preprocessed_jd_text = preprocess_text(jd_text)\n",
    "\n",
    "    similarity_score = calculate_similarity(' '.join(preprocessed_resume_text), ' '.join(preprocessed_jd_text))\n",
    "    print(\"The similarity score between the resume and job description is: \", similarity_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "dcad5474b6ca0f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between the resume and job description is:  0.32852134033908575\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:30:53.120725Z",
     "start_time": "2024-11-29T14:30:24.622264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load required models\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # spaCy for preprocessing and NER\n",
    "keybert_model = KeyBERT()  # KeyBERT for skill and keyword extraction\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # SBERT for similarity matching\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_file = open(file_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in range(len(pdf_reader.pages)):\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        text += page_obj.extract_text()\n",
    "    pdf_file.close()\n",
    "    return text\n",
    "\n",
    "# Step 1: Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Step 2: Skill and Keyword Extraction Function\n",
    "def extract_keywords(text):\n",
    "\n",
    "    keywords = keybert_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=100)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "\n",
    "# Step 3: Semantic Similarity Function\n",
    "def calculate_similarity(resume_text, job_description):\n",
    "    # Generate embeddings\n",
    "    embeddings = sbert_model.encode([resume_text, job_description], convert_to_tensor=True)\n",
    "    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity_score.item()  # Return as a scalar\n",
    "\n",
    "\n",
    "# Step 4: Scoring Function\n",
    "def calculate_ats_score(resume_text, job_description):\n",
    "    # Preprocess texts\n",
    "    clean_resume = preprocess_text(resume_text)\n",
    "    clean_job_description = preprocess_text(job_description)\n",
    "\n",
    "    # Extract skills/keywords\n",
    "    resume_keywords = extract_keywords(clean_resume)\n",
    "    job_keywords = extract_keywords(clean_job_description)\n",
    "\n",
    "    # Calculate skill similarity\n",
    "    skill_similarity = calculate_similarity(\" \".join(resume_keywords), \" \".join(job_keywords))\n",
    "\n",
    "    # Calculate overall similarity\n",
    "    overall_similarity = calculate_similarity(clean_resume, clean_job_description)\n",
    "\n",
    "    # Weighted scoring\n",
    "    ats_score = (0.5 * skill_similarity) + (0.3 * overall_similarity) + (0.2 * overall_similarity)\n",
    "    return round(ats_score * 100, 2)  # Scale to percentage\n"
   ],
   "id": "8f24074fe2db0279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATS Score: 8.29%\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T12:38:58.265454Z",
     "start_time": "2024-11-29T12:38:50.377681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load required models\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # spaCy for preprocessing and NER\n",
    "keybert_model = KeyBERT()  # KeyBERT for skill and keyword extraction\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # SBERT for similarity matching\n",
    "\n",
    "\n",
    "# Step 1: Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Step 2: Skill and Keyword Extraction Function\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extracts key skills/keywords using KeyBERT.\n",
    "    \"\"\"\n",
    "    keywords = keybert_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "\n",
    "# Step 3: Semantic Similarity Function\n",
    "def calculate_similarity(resume_text, job_description):\n",
    "    \"\"\"\n",
    "    Calculates the semantic similarity using SBERT embeddings.\n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    embeddings = sbert_model.encode([resume_text, job_description], convert_to_tensor=True)\n",
    "    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity_score.item()  # Return as a scalar\n",
    "\n",
    "\n",
    "# Step 4: Scoring Function\n",
    "def calculate_ats_score(resume_text, job_description):\n",
    "    \"\"\"\n",
    "    Calculates the ATS score by combining skills, experience, and education similarity.\n",
    "    \"\"\"\n",
    "    # Preprocess texts\n",
    "    clean_resume = preprocess_text(resume_text)\n",
    "    clean_job_description = preprocess_text(job_description)\n",
    "\n",
    "    # Extract skills/keywords\n",
    "    resume_keywords = extract_keywords(clean_resume)\n",
    "    job_keywords = extract_keywords(clean_job_description)\n",
    "\n",
    "    # Calculate skill similarity\n",
    "    skill_similarity = calculate_similarity(\" \".join(resume_keywords), \" \".join(job_keywords))\n",
    "\n",
    "    # Calculate overall similarity\n",
    "    overall_similarity = calculate_similarity(clean_resume, clean_job_description)\n",
    "\n",
    "    # Weighted scoring\n",
    "    ats_score = (0.5 * skill_similarity) + (0.3 * overall_similarity) + (0.2 * overall_similarity)\n",
    "    return round(ats_score * 100, 2)  # Scale to percentage\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input: Resume text and Job description\n",
    "    resume = \"\"\"\n",
    "    John Doe\n",
    "    Experienced data scientist with expertise in Python, machine learning, and deep learning. \n",
    "    Proficient in tools like TensorFlow, PyTorch, and scikit-learn. \n",
    "    Strong analytical skills with a background in mathematics and statistics. \n",
    "    Certified AWS Solutions Architect and Google Cloud Engineer.\n",
    "    \"\"\"\n",
    "    \n",
    "    job_description = \"\"\"\n",
    "    We are looking for a data scientist proficient in Python and machine learning. \n",
    "    Candidates should have experience with deep learning frameworks like TensorFlow or PyTorch \n",
    "    and a strong understanding of statistics. Cloud certification (AWS or GCP) is a plus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate ATS Score\n",
    "    ats_score = calculate_ats_score(resume, job_description)\n",
    "    print(f\"ATS Score: {ats_score}%\")\n"
   ],
   "id": "35e2084259c20296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATS Score: 76.37%\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Amazon is at the forefront of innovative technology, driving progress in AI and machine learning solutions. Our mission is to leverage advanced data science and engineering to deliver impactful and scalable products for our clients. We are looking for a passionate Machine Learning Engineer to join our dynamic team and contribute to cutting-edge projects.\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Design, develop, and deploy scalable machine learning models and algorithms for a variety of applications.\n",
    "Collaborate with cross-functional teams, including data scientists, software engineers, and product managers, to gather requirements and implement solutions.\n",
    "Preprocess and analyze large datasets, ensuring data quality and feature engineering for optimal model performance.\n",
    "Build and maintain machine learning pipelines for training, evaluation, and deployment.\n",
    "Optimize and fine-tune models for performance, scalability, and accuracy using techniques such as hyperparameter tuning and model compression.\n",
    "Conduct thorough testing and validation of models to ensure reliability and robustness in production.\n",
    "Monitor and maintain deployed models, implementing strategies for model retraining and performance tracking.\n",
    "Stay current with the latest advancements in machine learning and AI, applying innovative techniques and technologies as appropriate.\n",
    "Document processes, model architectures, and code to ensure maintainability and knowledge sharing within the team.\n",
    "Qualifications:\n",
    "\n",
    "Bachelor's or Master's degree in Computer Science, AI/ML, Data Science, or a related field.\n",
    "Strong proficiency in Python and experience with libraries/frameworks such as TensorFlow, PyTorch, or Scikit-Learn.\n",
    "Solid understanding of machine learning algorithms, neural networks, and deep learning architectures.\n",
    "Experience with data preprocessing and feature engineering.\n",
    "Hands-on experience with cloud platforms (AWS, GCP, or Azure) for deploying machine learning models.\n",
    "Familiarity with MLOps practices, including version control, CI/CD pipelines, and model monitoring.\n",
    "Excellent problem-solving skills and the ability to work collaboratively in a team environment.\n",
    "Knowledge of big data tools (e.g., Spark, Hadoop) and database technologies is a plus.\n",
    "Nice-to-Have:\n",
    "\n",
    "Experience in Natural Language Processing (NLP) and working with transformers and LLMs.\n",
    "Understanding of computer vision and related frameworks.\n",
    "Contributions to open-source ML projects or participation in hackathons.\n",
    "Perks and Benefits:\n",
    "\n",
    "Competitive salary and performance-based bonuses.\n",
    "Flexible working hours and remote work options.\n",
    "Access to training programs, conferences, and certifications for continuous learning.\n",
    "Comprehensive health and wellness benefits."
   ],
   "id": "c871f0f7f16403aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
